{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run evaluation on different action policies, e.g. VLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hbuurmei/Documents/VLABench/.venv/lib/python3.11/site-packages/dash/_jupyter.py:30: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  _dash_comm = Comm(target_name=\"dash\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"MUJOCO_GL\"] = \"egl\"\n",
    "from VLABench.evaluation.evaluator import Evaluator\n",
    "from VLABench.evaluation.model.policy.openvla import OpenVLA\n",
    "from VLABench.evaluation.model.policy.base import RandomPolicy\n",
    "from VLABench.tasks import *\n",
    "from VLABench.robots import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_tasks = [\"select_fruit\"]\n",
    "unseen = True\n",
    "save_dir = \"/home/hbuurmei/Documents/VLABench/logs\"\n",
    "\n",
    "model_ckpt = \"openvla/openvla-7b\"\n",
    "lora_ckpt = \"VLABench/openvla-lora\"  # \"/remote-home1/pjliu/openvla/weights/select_fruit+CSv1+lora/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the task episodes by seeds, instead of episodes\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator(\n",
    "    tasks=demo_tasks,\n",
    "    n_episodes=2,\n",
    "    max_substeps=10,   \n",
    "    save_dir=save_dir,\n",
    "    visualization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load basic random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating select_fruit of RandomPolicy:   0%|          | 0/2 [00:00<?, ?it/s]WARNING:absl:Failed to converge after 99 steps: err_norm=0.0392998\n",
      "Evaluating select_fruit of RandomPolicy:  50%|█████     | 1/2 [00:35<00:35, 35.29s/it]WARNING:absl:Failed to converge after 99 steps: err_norm=0.00463179\n",
      "WARNING:absl:Failed to converge after 99 steps: err_norm=0.108446\n",
      "WARNING:absl:Failed to converge after 99 steps: err_norm=0.0497491\n",
      "WARNING:absl:Failed to converge after 99 steps: err_norm=0.0760256\n",
      "WARNING:absl:Failed to converge after 99 steps: err_norm=0.0871204\n",
      "WARNING:absl:Failed to converge after 99 steps: err_norm=0.19167\n",
      "WARNING:absl:Failed to converge after 99 steps: err_norm=0.680236\n",
      "WARNING:absl:Failed to converge after 99 steps: err_norm=0.103099\n",
      "WARNING:absl:Failed to converge after 99 steps: err_norm=0.10525\n",
      "WARNING:absl:Failed to converge after 99 steps: err_norm=0.0560078\n",
      "WARNING:absl:Failed to converge after 99 steps: err_norm=0.193755\n",
      "WARNING:absl:Failed to converge after 99 steps: err_norm=0.0435276\n",
      "WARNING:absl:Failed to converge after 99 steps: err_norm=0.0938221\n",
      "Evaluating select_fruit of RandomPolicy: 100%|██████████| 2/2 [01:45<00:00, 52.97s/it]\n"
     ]
    }
   ],
   "source": [
    "random_policy = RandomPolicy(model=None)\n",
    "result = evaluator.evaluate(random_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load policies, take OpenVLA as example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'openvla/openvla-7b/config.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m policy = \u001b[43mOpenVLA\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_ckpt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_ckpt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlora_ckpt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_ckpt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnorm_config_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mVLABENCH_ROOT\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconfigs/model/openvla_config.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m result = evaluator.evaluate(policy)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/VLABench/VLABench/evaluation/model/policy/openvla.py:47\u001b[39m, in \u001b[36mOpenVLA.__init__\u001b[39m\u001b[34m(self, model_ckpt, lora_ckpt, attn_implementation, norm_config_file, device, **kwargs)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[33;03mparam:\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[33;03m    model_ckpt: path to the model checkpoint\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     44\u001b[39m \u001b[33;03m    device: cuda device to run\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m norm_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[43mcopy_file_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm_config_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_ckpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconfig.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;28mself\u001b[39m.processor = AutoProcessor.from_pretrained(model_ckpt, trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     49\u001b[39m model = AutoModelForVision2Seq.from_pretrained(\n\u001b[32m     50\u001b[39m     model_ckpt,\n\u001b[32m     51\u001b[39m     \u001b[38;5;66;03m# attn_implementation=attn_implementation,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     54\u001b[39m     trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     55\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/VLABench/VLABench/evaluation/model/policy/openvla.py:22\u001b[39m, in \u001b[36mcopy_file_content\u001b[39m\u001b[34m(content_file, target_file)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(content_file, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     21\u001b[39m     content = f.read()\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtarget_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     23\u001b[39m     f.write(content)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'openvla/openvla-7b/config.json'"
     ]
    }
   ],
   "source": [
    "policy = OpenVLA(\n",
    "    model_ckpt=model_ckpt,\n",
    "    lora_ckpt=lora_ckpt,\n",
    "    norm_config_file=os.path.join(os.getenv(\"VLABENCH_ROOT\"), \"configs/model/openvla_config.json\")\n",
    ")\n",
    "\n",
    "result = evaluator.evaluate(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run evaluation on different VLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VLABench.evaluation.model.vlm import *\n",
    "from VLABench.evaluation.evaluator import VLMEvaluator\n",
    "\n",
    "vlm_name = \"GPT_4v\" # valid names: [\"GPT_4v\", \"Qwen2_VL\", \"InternVL2\", \"MiniCPM_V2_6\", \"GLM4v\", \"Llava_NeXT\"]\n",
    "fewshot_num = 0\n",
    "task_list = [\"mesh_and_texture/select_fruit\"]\n",
    "\n",
    "def initialize_model(model_name, *args, **kwargs):\n",
    "    cls = globals().get(model_name)\n",
    "    if cls is None:\n",
    "        raise ValueError(f\"Model '{model_name}' not found in the current namespace.\")\n",
    "    \n",
    "    return cls(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlm = initialize_model(vlm_name)\n",
    "evaluator = VLMEvaluator(\n",
    "    tasks=task_list,\n",
    "    n_episodes=2,\n",
    "    data_path=os.path.join(os.getenv(\"VLABENCH_ROOT\"), \"../dataset\", \"vlm\"),\n",
    "    save_path=os.path.join(os.getenv(\"VLABENCH_ROOT\"), \"../logs/vlm\"),\n",
    ")\n",
    "\n",
    "evaluator.evaluate(vlm, few_shot_num=fewshot_num)\n",
    "result=evaluator.get_final_score_dict(vlm_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlabench (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
