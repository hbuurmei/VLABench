{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Load Task in Benchmark Quickly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial one is to load a task/env conveniently by simply giving task name and desired target embodiment (default is Franka)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hbuurmei/Documents/VLABench/.venv/lib/python3.11/site-packages/glfw/__init__.py:917: GLFWError: (65550) b'X11: The DISPLAY environment variable is missing'\n",
      "  warnings.warn(message, GLFWError)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from dm_control import viewer\n",
    "from VLABench.envs import load_env\n",
    "from VLABench.robots import *\n",
    "from VLABench.tasks import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the embodiment and task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"select_fruit\"\n",
    "robot = \"franka\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If run this tutorial on server instead of PC, run in headless mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MUJOCO_GL\"] = \"egl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.1Load task and build environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each time you load the single env, each instance varies in a large range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hbuurmei/Documents/VLABench/.venv/lib/python3.11/site-packages/glfw/__init__.py:917: GLFWError: (65537) b'The GLFW library is not initialized'\n",
      "  warnings.warn(message, GLFWError)\n"
     ]
    },
    {
     "ename": "FatalError",
     "evalue": "gladLoadGL error",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFatalError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m env = load_env(task, robot=robot, time_limit=\u001b[32m1000\u001b[39m)\n\u001b[32m      2\u001b[39m env.reset()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m image = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcamera_id\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m640\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m640\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m Image.fromarray(image)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/VLABench/VLABench/envs/dm_env.py:33\u001b[39m, in \u001b[36mLM4ManipDMEnv.render\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mphysics\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/VLABench/.venv/lib/python3.11/site-packages/dm_control/mujoco/engine.py:223\u001b[39m, in \u001b[36mPhysics.render\u001b[39m\u001b[34m(self, height, width, camera_id, overlays, depth, segmentation, scene_option, render_flag_overrides, scene_callback)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrender\u001b[39m(\n\u001b[32m    179\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    180\u001b[39m     height=\u001b[32m240\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    189\u001b[39m                                       \u001b[38;5;28;01mNone\u001b[39;00m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    190\u001b[39m ):\n\u001b[32m    191\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Returns a camera view as a NumPy array of pixel values.\u001b[39;00m\n\u001b[32m    192\u001b[39m \n\u001b[32m    193\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    221\u001b[39m \u001b[33;03m    The rendered RGB, depth or segmentation image.\u001b[39;00m\n\u001b[32m    222\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m   camera = \u001b[43mCamera\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m      \u001b[49m\u001b[43mphysics\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m      \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m      \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcamera_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcamera_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m      \u001b[49m\u001b[43mscene_callback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscene_callback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    229\u001b[39m   image = camera.render(\n\u001b[32m    230\u001b[39m       overlays=overlays, depth=depth, segmentation=segmentation,\n\u001b[32m    231\u001b[39m       scene_option=scene_option, render_flag_overrides=render_flag_overrides)\n\u001b[32m    232\u001b[39m   camera._scene.free()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/VLABench/.venv/lib/python3.11/site-packages/dm_control/mujoco/engine.py:711\u001b[39m, in \u001b[36mCamera.__init__\u001b[39m\u001b[34m(self, physics, height, width, camera_id, max_geom, scene_callback)\u001b[39m\n\u001b[32m    708\u001b[39m \u001b[38;5;28mself\u001b[39m._rgb_buffer = np.empty((\u001b[38;5;28mself\u001b[39m._height, \u001b[38;5;28mself\u001b[39m._width, \u001b[32m3\u001b[39m), dtype=np.uint8)\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._depth_buffer = np.empty((\u001b[38;5;28mself\u001b[39m._height, \u001b[38;5;28mself\u001b[39m._width), dtype=np.float32)\n\u001b[32m--> \u001b[39m\u001b[32m711\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_physics\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontexts\u001b[49m.mujoco \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    712\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._physics.contexts.gl.make_current() \u001b[38;5;28;01mas\u001b[39;00m ctx:\n\u001b[32m    713\u001b[39m     ctx.call(mujoco.mjr_setBuffer, mujoco.mjtFramebuffer.mjFB_OFFSCREEN,\n\u001b[32m    714\u001b[39m              \u001b[38;5;28mself\u001b[39m._physics.contexts.mujoco.ptr)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/VLABench/.venv/lib/python3.11/site-packages/dm_control/mujoco/engine.py:533\u001b[39m, in \u001b[36mPhysics.contexts\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._contexts_lock:\n\u001b[32m    532\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._contexts:\n\u001b[32m--> \u001b[39m\u001b[32m533\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_rendering_contexts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._contexts\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/VLABench/.venv/lib/python3.11/site-packages/dm_control/mujoco/engine.py:519\u001b[39m, in \u001b[36mPhysics._make_rendering_contexts\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    516\u001b[39m render_context = _render.Renderer(\n\u001b[32m    517\u001b[39m     max_width=max_width, max_height=max_height)\n\u001b[32m    518\u001b[39m \u001b[38;5;66;03m# Create the MuJoCo context.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m mujoco_context = \u001b[43mwrapper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMjrContext\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[38;5;28mself\u001b[39m._contexts = Contexts(gl=render_context, mujoco=mujoco_context)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/VLABench/.venv/lib/python3.11/site-packages/dm_control/mujoco/wrapper/core.py:608\u001b[39m, in \u001b[36mMjrContext.__init__\u001b[39m\u001b[34m(self, model, gl_context, font_scale)\u001b[39m\n\u001b[32m    606\u001b[39m \u001b[38;5;28mself\u001b[39m._gl_context = gl_context\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m gl_context.make_current() \u001b[38;5;28;01mas\u001b[39;00m ctx:\n\u001b[32m--> \u001b[39m\u001b[32m608\u001b[39m   ptr = \u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmujoco\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMjrContext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfont_scale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    609\u001b[39m   ctx.call(mujoco.mjr_setBuffer, mujoco.mjtFramebuffer.mjFB_OFFSCREEN, ptr)\n\u001b[32m    610\u001b[39m gl_context.keep_alive(ptr)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/VLABench/.venv/lib/python3.11/site-packages/dm_control/_render/executor/render_executor.py:138\u001b[39m, in \u001b[36mPassthroughRenderExecutor.call\u001b[39m\u001b[34m(self, func, *args, **kwargs)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, *args, **kwargs):\n\u001b[32m    137\u001b[39m   \u001b[38;5;28mself\u001b[39m._check_locked()\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFatalError\u001b[39m: gladLoadGL error"
     ]
    }
   ],
   "source": [
    "env = load_env(task, robot=robot, time_limit=1000)\n",
    "env.reset()\n",
    "\n",
    "image = env.render(camera_id=2, width=640, height=640)\n",
    "Image.fromarray(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs=env.get_observation()\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2Run in interactive viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.launch(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Overview of Primitive Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import textwrap\n",
    "\n",
    "def plot_images(images, instructions, titles, max_columns=5, max_text_width=30):\n",
    "    assert len(images) == len(instructions) == len(titles), \"images should have the same length as instructions and titles\"\n",
    "    num_images = len(images)\n",
    "    num_rows = (num_images + max_columns - 1) // max_columns\n",
    "    fig, axes = plt.subplots(num_rows, max_columns, figsize=(max_columns * 3, num_rows * 3))\n",
    "    axes = axes.flatten()\n",
    "    for i, (image, instruction, title) in enumerate(zip(images, instructions, titles)):\n",
    "        if np.max(image) <= 1:\n",
    "            image = (image * 255).astype(np.uint8)\n",
    "        ax = axes[i]\n",
    "        ax.imshow(image)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(title, fontsize=14)\n",
    "        wrapped_instruction = textwrap.fill(instruction, width=max_text_width)\n",
    "        ax.text(0.5, -0.1, wrapped_instruction, ha='center', va='top', transform=ax.transAxes, fontsize=12)\n",
    "        \n",
    "    for i in range(num_images, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_env_and_render(task, robot, **kwargs):\n",
    "    env = load_env(task, robot=robot, **kwargs)\n",
    "    env.reset()\n",
    "\n",
    "    image = env.render(camera_id=2, width=480, height=480)\n",
    "    instruction = env.task.get_instruction()\n",
    "    return image, instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Basic Scene: Mesh&Texture "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default Setting: no additional domain randomization, no texture and scene augmenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_to_load = [\"select_toy\", \"select_fruit\", \"select_chemistry_tube\", \"select_mahjong\", \"select_poker\", \"add_condiment\", \"insert_flower\", \"select_book\", \"select_billiards\", \"select_drink\", \"select_ingredient\", \"select_painting\",\"put_box_on_painting\"]\n",
    "images, instructions = [], []\n",
    "for task in tasks_to_load:\n",
    "    try:\n",
    "        image, instruction = load_env_and_render(task, robot, reset_wait_step=0)\n",
    "        images.append(image)\n",
    "        instructions.append(instruction)\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(f\"Failed to load task {task}: {e}\")\n",
    "plot_images(images, instructions, titles=tasks_to_load, max_columns=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Common Sense & World Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base environment for Common Sense & World Knowledge type tasks quering LLM to generate task instructions by default. We also recommend to generate the instructions in batch later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_to_load = [\"hammer_loose_nail\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Spatial Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base environment for spatial understanding generate the relative spatial relationships between target entity and other entities by task-specific rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_to_load = [\"select_toy_spatial\", \"select_fruit_spatial\", \"select_chemistry_tube_spatial\", \"select_mahjong_spatial\", \"select_poker_spatial\", \"add_condiment_spatial\", \"insert_flower_spatial\", \"select_book_spatial\", \"select_billiards_spatial\", \"select_drink_spatial\", \"select_ingredient_spatial\"]\n",
    "# \"\", \"select_poker\", \"add_condiment\", \"insert_flower\", \"select_book\", \"select_billiards\", \"select_drink\", \"select_ingredient\", \"select_painting\",\"put_box_on_painting\"\n",
    "images, instructions = [], []\n",
    "robot=\"franka\"\n",
    "for task in tasks_to_load:\n",
    "    try:\n",
    "        image, instruction = load_env_and_render(task, robot, reset_wait_step=0)\n",
    "        images.append(image)\n",
    "        instructions.append(instruction)\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(f\"Failed to load task {task}: {e}\")\n",
    "plot_images(images, instructions, titles=tasks_to_load, max_columns=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Semantic Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_to_load = [\"select_toy_semantic\", \"select_fruit_semantic\"]\n",
    "for task in tasks_to_load:\n",
    "    try:\n",
    "        image, instruction = load_env_and_render(task, robot, reset_wait_step=0)\n",
    "        images.append(image)\n",
    "        instructions.append(instruction)\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(f\"Failed to load task {task}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Overview of Composite Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1Cluster series tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_to_load = [\"cluster_book\", \"cluster_billiards\", \"cluster_toy\", \"cluster_dessert\", \"cluster_drink\", \"cluster_ingredients\"]\n",
    "images, instructions = [], []\n",
    "for task in tasks_to_load:\n",
    "    image, instruction = load_env_and_render(task, robot, reset_wait_step=0)\n",
    "    images.append(image)\n",
    "    instructions.append(instruction)\n",
    "plot_images(images, instructions, titles=tasks_to_load, max_columns=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Other Composite Tasks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_to_load = [\"texas_holdem\", \"texas_holdem_explore\", \"set_dining_table\", \"set_dining_left_hand\", \"set_dining_chopstick\", \"find_unseen_object\", \"cool_drink\", \"take_out_cool_drink\", \"book_rearrange\", \"heat_food\", \"rearrange_tube\", \"take_chemistry_experiment\", \"get_coffee\", \"get_coffee_with_sugar\", \"get_coffee_with_milk\", \"hammer_nail_and_hang_picture\", \"make_juice\", \"cook_dishes\", \"store_food\", \"play_mahjong\", \"complex_seesaw_use\", \"play_math_game\", \"set_study_table\"]\n",
    "images, instructions = [], []\n",
    "for task in tasks_to_load:\n",
    "    image, instruction = load_env_and_render(task, robot, reset_wait_step=0)\n",
    "    images.append(image)\n",
    "    instructions.append(instruction)\n",
    "plot_images(images, instructions, titles=tasks_to_load, max_columns=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlabench (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
